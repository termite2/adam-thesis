\chapter{Background}

\section{$\mu$-calculus}

\subsection{Syntax}

\todo{Description of mu calculus includes the $[a]\phi$ bullshit that we dont need. It also doesnt have quantification. Perhaps we should not call it mu calculus and instead define a first order logic with fixpoints}

The $\mu$-calculus is a logic capable of expressing greatest and least solutions of fixed point equations $X = f(X)$ where $f$ is a monotone function. The set of formulas of $\mu$-calculus is defined as follows:

Let $P$ be a set of propositions, $A$ be a set of actions and $V$ be a set of variables. Then
\begin{itemize}
    \item each proposition $p$ and each variable is a formula
    \item if $\phi$ and $\psi$ are formulas then $\phi \wedge \psi$ is a formula
    \item if $\phi$ is a formula then $\neg \phi$ is a formula
    \item if $\phi$ is a formula and $a$ is an action then $[a]\phi$ is a formula
    \item if $\phi$ is a formula and $Z$ a variable then $\nu Z.\phi$ is a formula provided that every free occurrence of $Z$ in $\phi$ occurs under an even number of negations
\end{itemize}

\noindent Given these definitions, we also have:
\begin{itemize}
    \item $\phi \vee \psi$ meaning $\neg (\phi \wedge \psi)$
    \item $\langle a \rangle \phi$ meaning $\neg [a] \neg \phi$
\item $\mu Z. \phi$ meaning $\neg \nu Z. \neg \phi [Z := \neg Z]$ where $\phi[Z := \neg Z]$ means substituting $\neg Z$ for all free occurrences of $Z$ in $\phi$
\end{itemize}

\subsection{Semantics}

Given a labelled transition system $(S, R, F)$ where
\begin{itemize}
    \item S is the set of states
    \item $R \subseteq S \times A \times S$ is the transition relation
    \item $F : P \rightarrow 2^S$ maps each proposition to the set of states where the proposition is true
\end{itemize}

\noindent A $\mu$-calculus formula is interpreted as follows:
\begin{itemize}
    \item $p$ holds in the set of states $F(p)$
    \item $\phi \wedge \psi$ holds in the set of states where both $\phi$ and $\psi$ hold
    \item $\neg \phi$ holds in every state where $\phi$ does not hold 
    \item $[a]\phi$ holds in a state $s$ if every $a$-transition leading out of $s$ leads to a state where $\phi$ holds
    \item $\nu Z. \phi$ holds in any set of states $T$ such that when the variable $Z$ is set to $T$ in $\phi$ then $\phi$ holds for all of $T$. It is the greatest fixpoint of $\phi$.
\end{itemize}

\subsection{From $\mu$-calculus formulas to algorithms}

It is straightforward to convert a $\mu$-calculus formula into an algorithm that returns the set of states for which the formula holds.

Greatest and least fixed points are computed by iteration. To compute the greatest fixed point of a function $f(x)$ i.e. $\nu X. f(X)$ we iterate $f$ starting with the universal set. As $f$ is monotonic, the result is guaranteed to grow on each iteration \todo{why}. Furthermore, if the set of states, $S$ is finite, this iteration must eventually converge to some set as $S$ being finite prevents the result from growing forever. 

We give the algorithm for computing the greatest fixpoint in Algorithm~\ref{alg:mu_greatest_fixpoint}. It assumes the existence of an abstract set datatype that supports set union, complementation and equality checking.

\begin{algorithm}
\begin{algorithmic}

\Function{MuSemantics}{$\phi$}
    \If {$\phi = p$} 
        \State\Return $F(p)$
    \ElsIf {$\phi = \psi \vee \rho$}
        \State\Return $\Call{MuSemantics}{\psi} \cup \Call{MuSemantics}{\rho}$
    \ElsIf {$\phi = \neg \psi$}
        \State\Return $S - \Call{MuSemantics}{\psi}$
    \ElsIf {$\phi = \nu X. \psi$}
        \State $Z \gets S$
        \Loop
            \State $Z' \gets \Call{MuSemantics}{\psi[X:=Z]}$
            \If {$Z' = Z$}
                \State\Return $Z$
            \EndIf
            \State $Z \gets Z'$
        \EndLoop
    \EndIf
\EndFunction

\end{algorithmic}
\caption{MuSemantics, given a $\mu$-calculus formula, returns the set of states that satisfy the formula.}
\label{a:mu_semantics}
\end{algorithm}

\section{$\omega$-regular languages}

\section{Two player games}

Two-player games are a useful formalism for reactive synthesis. Many problems in electronic design automation, industrial automation and robotics can be formalised as a game. In particular, the driver synthesis problem can be formalised as a game, and, this is the formalism around which Termite is built. Here, we present the fundamentals of two player games. 

\subsection{Formalism}

A two player game is played by player~1 against its opponent, player~2. It consists of a possibly infinite state space $S$ on which the game is played. The game is always in some state $s \in S$ called the \emph{current state}. The game progresses from state to state according to a transition relation, $\delta \subseteq S \times L \times S$ where $S$ is the set of states and $L$ is a set of label variables. A transition $t \in S \times L \times S$ is allowed in the game iff $t \in \delta$. 

The meaning of the label $l \in L$ depends on the type of game, but for now we will consider turn based games. In a turn based game, $S$ is partitioned into two sets: the player~1 set $\tau_1$ and the player~2 set $\tau_2$, where $\tau_1 \cap \tau_2 = \emptyset$ and $ \tau_1 \cup \tau_2 = S$. When $s \in \tau_1$ player 1 gets to pick $l$ and when $s \in \tau_2$ player 2 gets to pick $l$. We refer to the opponent of player $i$ as $\overline{i}$ ($\overline{1} = 2$, $\overline{2} = 1$).

Lastly, each game has an associated set of initial states $I \in 2^S$ where execution of the game begins.

Putting this all together, we can identify a \emph{turn based game structure} $G = \langle S,L,I,\tau_1,\tau_2,\delta \rangle$ with a turn based game.

A game proceeds in an infinite sequence of rounds, starting from some initial state. The infinite sequence of states visited $(s_0, s_1,\ldots) \in S^\omega$ is called a path. An \emph{objective} $\Phi \subseteq S^\omega$ is a subset of state sequences of $G$. We are concerned with $\omega$-regular objectives, i.e., objectives characterised by $\omega$-regular languages \cite{omega_reg_lang}. 

A \emph{strategy} for player~$i$ is a function $\pi_i : S^* \times \tau_i \rightarrow L$ that, in any player~$i$ state, associates the history of the game with a label to play. The set of initial states $I$ and a player~$i$ strategy $\pi_i$ determines a set $Outcomes_i(I, \pi_i)$ of paths $s_0, s_1, s_2, \ldots $ such that $s_0 \in I$ and $s_{k+1} = \delta(s_k, \pi_i(s_0,\ldots,s_k))$ when $s_k \in \tau_i$ and $s_{k+1} = \delta(s_k, l)$ for some $l$ when $s_k \in \tau_{\overline{i}}$.  Given an objective $\Phi \subseteq S^\omega$ we say that state $s \in S$ is winning for player $i$ if there is a strategy $\pi_i$ such that $Outcomes_i({s}, \pi_i) \subseteq \Phi$. That is, if, by picking suitable labels, they can force the path to be within the set of winning sequences. An arbitrary set of infinite sequences is an extremely general, but not practically useful, way of defining an objective. In the following sections we will consider some more restricted objectives that have practical uses.

\subsection{Safety and reachability}

The two simplest objectives are safety and reachability. A safety objective is defined by a set $\safeobj \subseteq S$ that player 1 must force the game to stay within, regardless of the labels that player 2 picks. Formally, a run is safe if 

\begin{equation}
\label{eqn:safeobj}
\forall i.\ s_i \in \safeobj
\end{equation}

The dual of a safety objective is a reachability objective. A reachability objective is defined by a set $\reachobj \subseteq S$ that player 1 must force the game to visit at least once, regardless of the labels that player 2 picks. Formally, a reachability run is winning if 

\begin{equation}
\label{eqn:reachobj}
\exists i.\ s_i \in \reachobj
\end{equation}

\paragraph{\buchi}
A \buchi\ objective is defined by a set $\buchiobj \subseteq S$ that player~1 must always be able to force execution of the game into. This differs from a reachability game in that the region must always be reachable, not just once. When it has been reached once, it must be reachable again, and so on. So, it must be reachable infinitely many times. Formally, a run is \buchi\ winning if 

\begin{equation}
\forall i.\ \exists j>i.\ s_j \in \buchiobj
\end{equation}

\paragraph{Generalized \buchi}
A Generalized \buchi\ objective is defined by a finite set of sets $\genbuchiobj \subseteq 2^S$. Player~1 must always be able to force execution into each set $\buchiobj \in \genbuchiobj$. Formally, a run satisfies a generalized \buchi\ objective if 

\begin{equation}
\forall i.\ \forall \buchiobj \in \genbuchiobj.\ \exists j>i.\ s_j \in \buchiobj
\end{equation}

\paragraph{Fairness}
Sometimes it is necessary to rule out invalid plays that are not easily ruled out by changing the state machine. As an example, consider a progress assumption that guarantees that the game will eventually leave a set of `pending' states. For this we use fairness conditions. A fairness condition is a set of states, the \emph{fair} set, which we may assume that all valid runs of the game eventually enter. Or, equivalently, a set of states, called the \emph{unfair} set which we may assume that all valid runs of the game eventually leave. If a spoiling strategy exists that results in an unfair run, i.e.\ it does not always, at some point in the future, enter the fair set, then it does not count. Formally, a run satisfies a fair reachability objective if 

\begin{equation}
\forall i.\ \exists j>i.\ s_j \in \fairobj \rightarrow \exists i.\ s_i \in \reachobj
\end{equation}

\paragraph{GR(1)}
A Generalized Reactive 1, or GR(1) \cite{gr1} objective, is a generalized \buchi\ objective with multiple fairness conditions. In practice, this turns out to be a very useful type of objective. Formally, given a set $FAIRS$ of fair sets of states, a run satisfies a GR(1) objective if 

\begin{multline}
\forall i.\ \forall \fairobj \in \genfairobj.\ \exists j>i.\ s_j \in \fairobj \rightarrow \\ \forall i.\ \forall \buchiobj \in \genbuchiobj.\ \exists j>i.\ s_j \in \buchiobj
\end{multline}

Informally, a run is winning in a GR(1) game if for all fair runs the generalised \buchi\ condition holds.

\subsection{Perfect information games}

Perfect information games are two player games where both players know the current state of the transition system at all times. We will assume that all games are perfect information games.

\subsection{Strategies and counterexamples}
\label{sec:strat_and_cex}

If a game is winnable for player~1 then there exists (by definition) a strategy that tells player~1, in any state, which label it must play and ensures that if player~1 adheres to the strategy then the objective will be satisfied.

It is known that for perfect information safety, reachability and \buchi\ games, if there exists a strategy, then there also exists one that only depends on the current state \cite{something}. Thus, we can simplify the definition of a strategy in this case for player~1 to be a function $\pi_1 : \tau_1 \rightarrow L$ that only depends on the current state.

Furthermore, it is known that for generalised \buchi\ and GR(1) games, if there exists a strategy, then there also exists one that only depends on the current state and some finite additional state \cite{something}. We can simplify the definition of a strategy in this case for player~1 to be a function $\pi_1 : \tau_1 \times \sigma \rightarrow L \times \sigma$ that only depends on the current state and the finite additional state ($\sigma$), and, in addition to returning the label to play, returns the updated additional state.

Perfect information $\omega$-regular games are \emph{determined} \todo{this is not true in our case because they are non-deterministic, do we use the simpler deterministic case here and fix this later?}. That is, if the game is losing for player~1 then the game is winning for player~2, i.e.\ there exists a strategy for player 2 to violate the objective \cite{something}.

This is known as a \emph{counterexample strategy}. It is a strategy for player~2 which ensure that player~1 cannot satisfy their objective. Again, in the case of perfect information safety, reachability and \buchi\ games, if there exists a counterexample strategy (or, equivalently if the game is not winnable for player~1) then there exists a counterexample strategy that only depends on the current state. This is a function $\pi_2 : \tau_2 \rightarrow L$ that associates any player~2 state with a label for player~2 to play. 

Again, in the case of generalised \buchi\ and GR(1) games, the counterexample strategy may depend on an additional finite state: $\pi_1 : \tau_2 \times \sigma \rightarrow L \times \sigma$.

\section{Solving games}

Given a game and an objective there are two questions we can ask:
\begin{itemize}
    \item can player~1 win?
    \item if so, what is the strategy for player~1 and, if not, what is the counterexample strategy for player~2
\end{itemize}

We present algorithms to answer these questions in this section. We begin with the simplest game, the reachability game, and progress to GR(1) games as are used in Termite.

\subsection{Controllable predecessor}

All of the algorithms for games I will be describing use a function called the \emph{controllable predecessor} (abbreviated \cpre). \cpre\ is a function from a target set of game states ($2^S$) called the \emph{target set} to another set of game states. Given a set $T$, $CPre(T)$ returns the set of states from which player 1 can force execution into $T$ in one step. 

The exact details of \cpre\ do not matter and here we consider it to be a parameter to the game solving algorithms. The function it computes depends on the application and I will be describing the driver synthesis \cpre\ later. For now, the only property of \cpre\ that we require is that it is monotonic, i.e.\ if $X \subseteq Y$ then $CPre(X) \subseteq CPre(Y)$. Clearly, any reasonable \cpre\ function will have this property.

However, in the interest of aiding the reader's understanding, the controllable predecessor for turn based games is given below:

\begin{multline}
CPre(X) = \tau_1 \cap \{x | \exists l. \langle x, l, x' \rangle \in \delta \rightarrow x' \in X\} \\ \cup \tau_2 \cap \{x | \forall l. \langle x, l, x' \rangle \in \delta \rightarrow x' \in X \}
\label{eqn:turn_cpre}
\end{multline}

The controllable predecessor for turn based games returns the subset of $\tau_1$ where there exists a label which player~1 can choose to take execution into $X$ together with the subset of $\tau_2$ where all labels which player~2 can choose take execution into $X$. 

\begin{figure}[t]
\centering
\includegraphics[width=0.2\linewidth]{diagrams/turnCpre.pdf}
\caption{Turn based controllable predecessor}
\label{fig:turn_cpre}
\end{figure}

This is illustrated in Figure~\ref{fig:turn_cpre}. The two dark states are winning and they form the set $X$ passed to $CPre$. $CPre$ return state $s1$ in the winning set because it is a controllable state and there exists a transition into a winning state. State $s2$ is also winning because it is an uncontrollable state and all outgoing transitions go to winning states. Lastly, $s3$ is \emph{not} winning because it is uncontrollable and one of its outgoing transitions does not lead to a winning state.

\subsection{Safety and reachability}

We will start with reachability as it is the most straightforward. We solve games by finding the set of states from which player~1 can win, called the \emph{winning region} and denoted \win. If the winning region contains the set of initial states, then we know that player~1 can win from any state in the initial set and thus can win the game.

According to Equation~\ref{eqn:reachobj}, a state is winning in a reachability game if there is some finite $i$ such that we can guarantee that after $i$ rounds of the game, execution will have, at least once, entered a state in $\reachobj$. 

We can define the winning region inductively. It is obviously possible to reach the set $\reachobj$ from $\reachobj$ in 0 steps as we are already there. This is the base case. It is also possible to reach $\reachobj$ from $x \in S$ in $N + 1$ steps or fewer iff there exists $Y\subseteq 2^S$ such that it is possible to reach $\reachobj$ from all of $Y \subseteq 2^S$ in $N$ steps or fewer and $x \in CPre(Y)$.

This suggests an algorithm to find the winning region. We start with $\reachobj$, apply \cpre\ to get the states winning in 1 step, combine with $\reachobj$ again to get the states reachable in 1 step or less, and then repeat. After $N$ iterations, we find the states winning in $N$ or less steps. The algorithm is given in Algorithm~\ref{a:reach} and the equivalent $\mu$-calculus formula is given in Equation~\ref{eqn:mu_reach}. Two questions remain: 

\begin{itemize}
    \item An algorithm must terminate, so, when should we stop iterating?
    \item After termination, has the algorithm found all the winning states?
\end{itemize}

We denote the set of states from which it is possible to reach \reach\ in $N$ or less steps $W_N$. Observe that $W_{N+1} \supseteq W_N$. So, on each iteration, we either grow the winning set or it remains the same. Also observe that our set $S$ is finite, which means that $2^S$ is also finite. As set inclusion is a transitive relation, we cannot grow our winning set forever, so eventually we must reach a fixed point of the \cpre\ function. 

%TODO: $CPre$ isnt the order preserving function
A more formal proof of termination is based in the Knaster-Tarski theorem \cite{knaster_tarski}. The power set of $S$ can be ordered by set inclusion to obtain a complete lattice with supremum $S$ and infimum $\emptyset$. \cpre\ is an order preserving function, so by the Knaster-Tarski theorem the set of fixed points of \cpre\ is also a complete lattice. Thus there exists a greatest and least fixed point (as the lattice is complete). The least fixed point of \cpre\ is clearly obtained by iteration starting from the least element of the lattice, ie. $\emptyset$. 

Thus, we will eventually reach a fixed point, and, this is when we should stop iterating as further iterations will not change the winning set. Furthermore, we know that after any finite number $N$ of iterations where $N$ is greater than the number of iterations required to reach the fixed point, the winning region will remain $WIN$. Thus $WIN_N = WIN$ and we have found all winning states.

Finally, to answer the question of whether the game is winning for player~1, we check if the winning region contains the initial state set. If it does, we return Yes; otherwise, we return No.

Safety games are the dual of reachability games and are also solved by iterating the controllable predecessor. The algorithm for solving safety games is also the dual of the algorithm for solving reachability games and will not be given here. However, we do give the $\mu$-calculus formula in Equation~\ref{eqn:mu_safe} and the algorithm can be deduced from this using Algorithm~\ref{a:mu_semantics}.

\begin{equation}
    \mathit{\reachFun}(T) = \mu Y. CPre(Y \vee T)
\label{eqn:mu_reach}
\end{equation}

\begin{equation}
\mathit{\safeFun}(T) = \nu Y. CPre(Y \wedge T)
\label{eqn:mu_safe}
\end{equation}

\begin{algorithm}[t]
\begin{algorithmic}

\Require A set of target states $\reachobj \subseteq S$, an initial set of states $I$ and a monotonic controllable predecessor $CPre$.
\Ensure  {\it Yes} if $I \subseteq \mathit{\reachFun(T, Cpre)}$ and {\it No} otherwise.

\Function{\reachFun}{$\reachobj, I, CPre$}
    \State $Y \gets \varnothing$
    \Loop
        \State $Y' \gets CPre(Y \cup \reachobj)$
        \If {$Y' = Y$} 
            \If {$Y \subseteq I$}
                \State\Return Yes
            \Else
                \State\Return No
            \EndIf
        \EndIf
        \State $Y \gets Y'$
    \EndLoop
\EndFunction

\end{algorithmic}
\caption{Solving a reachability game}
\label{a:reach}
\end{algorithm}

\subsection{\buchi}

To solve a \buchi\ game, we first find the set from which we can reach the goal once as we do for a reachability game. Then, we use this set to find the set from which we can reach the goal twice, three times, and so on until we get to another fixed point. 

Imagine we have solved the reachability game $R = \reachFun(T)$ where $T$ is the \buchi\ target set and $R$ is the winning region. Then $V = R \wedge T$ is a subset of the goal from which player~1 can reach the goal one more time. Computing $W = \reachFun(V)$ gets us the set of states from which we can reach the goal twice. Iterating this procedure will eventually lead to a fixed point as $\reachFun$ is a monotonic function.

This fixed point is the set of states from which we can reach the goal any number of times. Thus, it is the winning set of the \buchi\ game. The $\mu$-calculus formula is given in Equation~\ref{eqn:mu_buchi} and the algorithm to solve \buchi\ games can be deduced from this using Algorithm~\ref{a:mu_semantics}.

\begin{equation}
    \mathit{\buchiFun}(T) = \nu X. \mu Y. CPre(Y \vee (X \wedge T))
\label{eqn:mu_buchi}
\end{equation}

\subsection{Generalized \buchi}

Solving a generalized \buchi\ game is similar to a \buchi\ game except that we find the set from which we can reach any goal once, then any goal twice, etc. The algorithm is a small modification to the \buchi\ algorithm and is given in Algorithm~\ref{alg:gen_buchi}. The equivalent $\mu$-calculus formula is given in Equation~\ref{eqn:mu_genbuchi}.

\begin{algorithm}[t]
\begin{algorithmic}
\Function{Generalized\_Buchi}{$T$, $CPre$}
\State $X \gets S$
\Loop
\State $X' \gets S$

\For{\textbf{each} G in Goals}
\State $X' \gets X' \cap \Call{\reachFun}{X \cap G}$
\EndFor

\If {$X' = X$} 
\State\Return $X$\EndIf
\State $X \gets X'$

\EndLoop
\EndFunction
\end{algorithmic}
\caption{Solving a generalized \buchi\ game}
\label{alg:gen_buchi}
\end{algorithm}

\begin{equation}
    \mathit{\genBuchiFun}(Goals) = \nu X. \bigwedge_{G \in Goals} \mu Y. CPre(Y \vee (X \wedge G))
\label{eqn:mu_genbuchi}
\end{equation}

\subsection{GR(1)}

Next, we need to add fairness. Consider a fair reachability game. An unfair region is a region that we can assume execution will leave, regardless of the loops it contains. We modify the controllable predecessor to create a fair controllable predecessor that takes this into account. Intuitively, the algorithm considers a set of unfair states to be winning if the only way out leads to an already winning state. To achieve this, we play a variation of a safety game where we can win if we stay indefinitely within the unfair set or upon exiting the unfair set we are immediately in the target set $T$. The procedure for the fair controllable predecessor is given in Algorithm~\ref{a:fair_cpre} and the equivalent $\mu$-calculus formula is given in Equation~\ref{eqn:mu_fair}. 

\begin{algorithm}[t]
\begin{algorithmic}
\Function{\fairCpreFun}{$CPre$, $\phi$, $T$}
\State $Z \gets S$
\Loop
\State $Z' \gets \Call{CPre}{Z \cap \phi \cup T}$
\If {$Z' = Z$} 
\State\Return $Z$\EndIf
\State $Z \gets Z'$
\EndLoop
\EndFunction
\end{algorithmic}
\caption{The fair controllable predecessor}
\label{a:fair_cpre}
\end{algorithm}

\begin{equation}
    \mathit{\fairCpreFun}(F, T) = \nu Z. CPre((\neg F \wedge Z) \vee T)
\label{eqn:mu_fair}
\end{equation}

Using $\mathit{\fairCpreFun}$ as the controllable predecessor operator in the reachability algorithm (Algorithm~\ref{a:reach}) yields the fair reachability algorithm. Finally, if we combine the \buchi\ game with the fair controllable predecessor we get a GR(1) game. The procedure is given in Algorithm~\ref{a:gr1} and the equivalent $\mu$-calculus formula is given in Equation~\ref{eqn:mu_gr1}.

\begin{algorithm}[t]
\begin{algorithmic}
\Function{\grWin}{$CPre$, $T$}
\State\Return \Call{\buchiFun}{$T$, \fairCpreFun(CPre)}
\EndFunction
\end{algorithmic}
\caption{GR(1) game}
\label{a:gr1}
\end{algorithm}

\begin{equation}
    \grWin(Goals, Fairs) = \nu X. \bigwedge_{G \in Goals} \mu Y. \bigvee_{F \in Fairs} \nu Z. CPre((\neg F \wedge Z) \vee (G \wedge X) \vee Y)
\label{eqn:mu_gr1}
\end{equation}

\section{Strategies and Counterexamples}

\subsection{Extracting strategies}

\subsubsection{Reachability}

Once we have solved a game and determined that it is winning, we can extract a strategy. In driver synthesis, the strategy is used to generate the driver. A strategy for a reachability game is a relation between states and labels for player~1 to play such that if player~1 sticks to this strategy, the game will eventually end up in the goal. Strategy extraction requires a straightforward modification to the game solving algorithm.

When solving a reachability game, we discover the sets of states for which we can force execution into the goal in 1 step, 2 steps, etc. When extracting a strategy for a reachability game we need to record, for each iteration, how we got one step closer to the goal. The algorithm is given in Algorithm~\ref{alg:reach_strat}. 

The strategy relation is initialized to the empty set on line~\ref{l:rs:init}. Then, we perform an iteration of the controllable predecessor. This time we also use \textsc{CPre\_Strat} on line~\ref{l:rs:cs}, which computes a relation between the winning states discovered so far and the labels that take execution from these newly discovered states one step closer to the goal.

On each iteration we combine this strategy for the newly discovered states with the strategy for the previously discovered states. We are careful on line~\ref{l:rs:as} not to add new labels for states that have already been discovered to ensure that the labels take us towards the goal. We iterate until our winning region reaches a fixed point as before. In the end, the strategy relates each state in the final winning region to a set of labels that take the game one step closer to the goal.

\textsc{CPre\_Strat} is a parameter to the strategy extraction algorithm in the same way as \textsc{\cpre} is to the game solving algorithm. Its definition depends on the exact details of the game. For completeness, we give the definition of \textsc{CPre\_Strat} for turn based games. 

\begin{equation}
    CPre\_Strat(X) = \{\langle x, l \rangle | x \in \tau_1 \wedge (\langle x, l, x' \rangle \in \delta \rightarrow x' \in X)\}
\end{equation}

It returns a set of $\langle state, label \rangle$ pairs where the state belongs to the \pone\ controllable set such that if the label is played, execution ends up in the set $X$, the argument to the function.

\begin{algorithm}[t]
    \begin{algorithmic}[1]

\Function{Reach\_Strat}{$\reachobj, CPre, CPre\_Strat$}
\State $Y \gets \varnothing$ \label{l:rs:init}
\State $STRAT \gets \varnothing$
    \Loop
        \State $Y' \gets CPre(Y \cup \reachobj)$
        \If {$Y' = Y$} 
            \State\Return $(Y, STRAT)$
        \EndIf
        \State $STRAT' \gets CPre\_Strat(Y \cup \reachobj)$ \label{l:rs:cs}
        \State $STRAT \gets STRAT \cup \{\langle s, l \rangle | \langle s, l \rangle \in STRAT' \wedge s \notin Y\}$  \label{l:rs:as}
        \State $Y \gets Y'$
    \EndLoop
\EndFunction

\end{algorithmic}
\caption{Extracting a strategy for a reachability game}
\label{alg:reach_strat}
\end{algorithm}

\subsubsection{\buchi}

Like a reachability strategy, a strategy for a \buchi\ game must ensure that we eventually get to the goal. However, it must also ensure that once we get to the goal it is still possible to reach the goal again. 

If we find a reachability strategy for the intersection of the goal set and the winning region, then when we follow this strategy, we are guaranteed to reach the goal once and then be able to reach it again by following the same strategy because we remain in the winning region. \todo{Do I need to explain why the intersection is not empty and why there exists a reachability strategy for it?} Thus, the algorithm for finding the strategy for a \buchi\ game is to find the winning region of the game and then compute the reachability strategy for the intersection of the goal and the winning region.

The algorithm for computing the strategy in a \buchi\ game is given in Algorithm~\ref{alg:buchi_strat}. It simply computes the \buchi\ winning region on line~\ref{l:bs:rw} and then computes the strategy to reach the intersection of the winning region and the goal set on line~\ref{l:bs:rs}.

\begin{algorithm}[t]
\begin{algorithmic}[1]

\Function{Buchi\_Strat}{$\buchiobj$, $\mathit{\cpre}$, $\mathit{\cpreStrat}$}
    \State $win \gets \Call{Buchi}{\buchiobj}$ \label{l:bs:rw}
    \State \Return \Call{Reach\_Strat}{$win \cap \buchiobj$, $\mathit{\cpre}$, $\mathit{\cpreStrat}$} \label{l:bs:rs}
\EndFunction

\end{algorithmic}
\caption{Extracting a strategy for a \buchi\ game}
\label{alg:buchi_strat}
\end{algorithm}

\subsubsection{Generalized \buchi}

A generalized \buchi\ strategy must ensure that we can always get to any of the goals. If we compute, for each goal, the reachability strategy for the intersection of that goal and the generalised \buchi\ winning region, we get a strategy that takes us to that goal and, by keeping execution in the winning region, ensures that we can reach any of the other goals using their strategies. 

Thus, a strategy for a generalised \buchi\ game is a set of strategies, one for each goal, that we must play in a round robin manner to ensure that each goal is eventually reached. The finite state required to ensure that the strategies are played in a round robin order is the finite state that the generalised \buchi\ strategy requires as discussed in Section~\ref{sec:strat_and_cex}. This, for example, could be a counter that that loops through the goals and is incremented each time the current goal is reached.

The algorithm for computing the strategy in a generalized \buchi\ game is given in Algorithm~\ref{alg:gen_buchi_strat}. It first computes the winning region of the generalised \buchi\ on line~\ref{l:gbs:wr} and then it computes a strategy to reach the intersection of this and each goal on line~\ref{l:gbs:rs}.

\begin{algorithm}
\begin{algorithmic}[1]

\Function{Gen\_Buchi\_Strat}{$\genbuchiobj$}
\State $win \gets \Call{Generalized\_Buchi}{\genbuchiobj}$ \label{l:gbs:wr}
    \For{\textbf{each} $G \in \genbuchiobj$}
        \State \Return \Call{Reach\_Strat}{$win \cap G$} \label{l:gbs:rs}
    \EndFor
\EndFunction

\end{algorithmic}
\caption{Extracting a strategy for a generalized \buchi\ game}
\label{alg:gen_buchi_strat}
\end{algorithm}

\subsubsection{Fairness}

To compute strategies for fair variants of reachability, \buchi\ and GR(1) games we use the same approach as solving them and introduce a modified controllable predecessor that takes fairness into account, called CPre\_Strat\_Fair. This modified controllable predecessor, in turn, is parameterised by the CPre and CPre\_Strat of the underlying game.

The fair controllable predecessor must ensure that we can reach the target set assuming that an unfair condition is not forever true. Conceptually, it tries to keep execution within an unfair region (as a safety game would) for which the only way out takes us a step closer to the goal. 

The algorithm for computing the strategy in a fair reachability game is given in Algorithm~\ref{alg:fair_cpre_strat}. It essentially solves a safety game for $\neg\phi$ with the exception that it is possible to leave safe regions as long as you enter $T$. When a fixed point is reached on line~\ref{l:fcp:st} it uses CPre\_Strat to compute a strategy to either stay within the computed unfair region or enter the target set. This strategy is guaranteed to reach the target set as long as fairness if not violated.

\begin{algorithm}
\begin{algorithmic}[1]

\Function{Fair\_CPre\_Strat}{$CPre, CPre\_Strat, \phi, T$}
    \State $Z \gets U$
    \Loop
        \State $Z' \gets \Call{CPre}{Z \wedge \neg\phi \vee T}$
        \If {$Z' = Z$} 
        \State\Return $\Call{CPre\_Strat}{Z \wedge \neg\phi \vee T}$ \label{l:fcp:st}
        \EndIf
        \State $Z \gets Z'$
    \EndLoop
\EndFunction

\end{algorithmic}
\caption{Fair controllable predecessor strategy extraction}
\label{alg:fair_cpre_strat}
\end{algorithm}

\subsubsection{GR(1)}

Strategies for GR(1) games, as used in Termite, can be computed using Algorithm~\ref{alg:gen_buchi_strat} instantiated with the fair controllable predecessor, Algorithm~\ref{alg:fair_cpre_strat}.

\subsection{Extracting counterexamples}

Counterexamples are computed by solving the complement game and extracting the strategy.

\subsubsection{Reachability}

The complement of a reachability game is a safety game with negated objective. This can be seen by complementing the $mu$-calculus formula for the winning region, repeated below for convenience.

\begin{equation}
    \mathit{REACH}(T) = \mu Y. CPre_1(Y \vee T)
\end{equation}

\begin{equation}
    \neg\mathit{REACH} = \neg\nu Y. CPre_2(Y \wedge \neg T) = \mathit{SAFE}(\neg T) 
\end{equation}

Thus, the counterexample strategy for a reachability game is the strategy for a safety game with complemented objective and the other player's controllable predecessor. \todo{We dont actually give an algorithm for safety game strategy, so this is a bit useless.}

\subsubsection{GR(1)}

The complement of a GR(1) game is given below.

\begin{equation}
    GR1(Goals, Fairs) = \nu X. \bigwedge_{G \in Goals} \mu Y. \bigvee_{F \in Fairs} \nu Z. CPre_1((\neg F \wedge Z) \vee (G \wedge X) \vee Y)
\end{equation}

\begin{equation}
    \neg GR1(Goals, Fairs) = \mu X. \bigvee_{G \in Goals} \nu Y. \bigwedge_{F \in Fairs} \mu Z. CPre_2((F \vee Z) \wedge (G \vee X) \wedge Y)
\end{equation}

\section{Symbolic games}
\label{sec:symbolic_games}

The algorithm described so far appears very inefficient. Consider a reachability game. We are performing a backwards breadth-first search starting from $\reachobj$. If we were to implement it directly as described, we would need a set abstract datatype to represent the winning set. Some of the games we have solved with Termite have upwards of $2^{80}$ states, even after abstraction. Clearly, explicitly representing the winning set will never succeed. 

Identical problems are encountered in model checking. The breakthrough that revolutionised model checking was to represent state sets implicitly as a characteristic equation over state variables.

\subsection{State variable encoding}

Symbolic games are defined over a finite set of state variables, $\symStateVars$, and a finite set of label variables $\symLabelVars$. A \emph{valuation} for a set of variables $V$ is a function from each of those variables to an element of its domain. We redefine $S$, the set of states, to be the set of possible valuations of each state variable in $\symStateVars$. That is, each state $s \in S$ is given by a valuation of all of the state variables in $\symStateVars$. Similarly, we redefine $L$, the set of labels, to be the set of possible valuations of each label variable in $\symLabelVars$.

For a set $Z$ of variables, we denote by $\forms(Z)$ the set of propositional formulas constructed from the variables in $Z$. The characteristic formula of a set of states $T$ is a function $f \in \forms(\symStateVars)$ that evaluates to true for the valuation corresponding to a state $s \in T$ and false otherwise. We use characteristic formulas to represent sets of states without explicitly listing each member of the set. This is called a symbolic representation.

Likewise, $\delta$ is specified by a formula in $\forms(\symStateVars \bigcup \symLabelVars \bigcup \symStateVars')$, where $\symStateVars' = \{\sigma' \mid \sigma \in \symStateVars \}$ is the set of next state variables.

\subsection{Symbolic algorithms}

We can redefine our game solving algorithms to use characteristic functions instead of explicit sets. The algorithms are superficially similar except they use conjunction and disjunction to modify sets instead of explicit set intersection and union. As an example, we convert the turn based controllable predecessor (Equation~\ref{eqn:turn_cpre}) to symbolic form in Equation~\ref{eqn:cpre_symb} and, we convert the simplest algorithm, determining the outcome of a reachability game to symbolic form, in Algorithm~\ref{a:symb_reach}.

\begin{equation}
CPre(X) = \tau_1 \wedge \exists \symLabelVars. \forall \symStateVars'. \delta \rightarrow X' \vee \tau_2 \wedge \forall \symLabelVars. \forall \symStateVars'. \delta \rightarrow X' 
\label{eqn:cpre_symb}
\end{equation}

\begin{algorithm}
\begin{algorithmic}

\Require The characteristic function of the set of target states $\reachobj \subseteq S$, the characteristic function of the initial set of states $I$ and a monotonic controllable predecessor \cpre\ that operates on characteristic functions.
\Ensure  {\it Yes} if $I \subseteq REACH(T, CPre)$ and {\it No} otherwise.

\Function{Reach}{$\reachobj, I, CPre$}
    \State $Y \gets False$
    \Loop
        \State $Y' \gets CPre(Y \vee \reachobj)$
        \If {$Y' = Y$} 
            \If {$Y \rightarrow I$}
                \State\Return Yes
            \Else
                \State\Return No
            \EndIf
        \EndIf
        \State $Y \gets Y'$
    \EndLoop
\EndFunction

\end{algorithmic}
\caption{Solving a reachability game symbolically}
\label{a:symb_reach}
\end{algorithm}

\subsection{Strategy generation}

\todo{Symbolic strategy generation: should I bother?}

\subsection{Binary decision diagrams}

A binary decision diagram is an efficient data structure for manipulating large propositional formulas. They are the symbolic data structure that we use in Termite.

\subsubsection{Binary decision trees}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\linewidth]{diagrams/binaryDecisTree.pdf}
\caption{A binary decision tree for $x \vee y$}
\label{fig:decis_tree}
\end{figure}

Figure~\ref{fig:decis_tree} shows a decision tree for the disjunction of two variables. The root node represents the disjunction function. The child nodes, or internal nodes, represent variables and the leaf nodes, or terminal nodes represent the outcome of the function. Terminal nodes are labelled either True or False. Given a valuation of X and Y, we can evaluate the function by starting at the root and taking the solid edge if the variable represented by the node is assigned to True in the valuation and taking the dashed edge if the value is assigned to False. 

For example, in Figure~\ref{fig:decis_tree}, for the valuation $x=True$ and $y=False$, we start at the root node which is labelled x as x is this node's decision variable. Variable $X$ is assigned to True so we follow the solid edge to the next decision node which is labelled y. Y is assigned to false so we take the dashed edge and arrive at a terminal node whose value is 1, meaning that the function evaluates to 1 for this variable valuation.

\subsubsection{Ordered binary decision trees}

If the order in which the variables appear along all paths starting from the root node and ending at a terminal node are the same, then the decision tree is called an ordered decision tree. The decision tree in Figure~\ref{fig:decis_tree} is ordered.

\subsubsection{Reduced ordered binary decision diagrams}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\linewidth]{diagrams/bdd.pdf}
\caption{A binary decision diagram}
\label{fig:bdd}
\end{figure}

A reduced ordered binary decision diagram (from now on, just BDD) is created by sharing subtrees as much as possible within an ordered binary decision tree.

In particular: 
\begin{itemize}
    \item Terminal nodes with the same label are merged. This means there are only two terminal nodes: True and False.
    \item Internal nodes with the same children are merged.
    \item Nodes with two identical children are removed and all incoming nodes are redirected to the child.
\end{itemize}

Figure~\ref{fig:bdd} is an example of a BDD.

\subsubsection{Complement arcs}

\subsubsection{Canonicity}
Given a variable ordering, reduced ordered binary decision diagrams are a canonical representation of a function. This means that given a function $f$ of some set $S$ of variables, another function $g$ that evaluates to the same value for each valuation of the variables in $S$ will be represented by exactly the same BDD.

In practice, one uses a BDD library such as CUDD \cite{cudd} to build and manipulate BDDs. CUDD keeps track of all BDDs and subtrees within the BDDs that have been created and reuses these to ensure that all BDDs remain in reduced form. This, along with canonicity, means that BDD equivalence can be checked in constant time simply by checking pointer equality of the two BDDs.

\subsubsection{Conjunction and disjunction}

BDDs are not usually built as decision trees and then reduced. Instead, they are built from the bottom up, starting with the terminal and variable nodes and combining these using conjunction, disjunction and negation.

Conjunction and disjunction are computed using a straightforward recursive algorithm that will not be given here, but if the reader is interested, more information can be found in \cite{somenzi_bdd}. An important result is that, in the worst case, the procedure runs in time proportional to the product of the sizes of the two input BDDs. Furthermore, the size of the resulting BDD may be equal to the product of the sizes of the two input BDDs in the worst case. A strength of BDDs, however, is that this worst case rarely happens in practice. 

\subsubsection{Function composition and quantification}

Function composition is where a BDD representing some function is substituted for a variable in another BDD.

Given a function $f(x_1,\cdots,x_n)$, we define existential quantification of $f$ with respect to the Boolean variable $x_i$ as $\exists x_i.\ f = f_{x_i} \vee f_{\overline{x_i}}$ and universal quantification of $f$ with respect to $x_i$ as $\forall x_i.\ f = f_{x_i} \wedge f_{\overline{x_i}}$.

Quantifications of the same type commute, so quantification with respect to a set of variables is well defined. 

\subsubsection{Variable ordering}

The number of nodes in a BDD depends drastically on the ordering chosen for the variables. Therefore, the space occupied by the BDDs and the time spent performing operations on them also depends on the variable ordering. This directly affects the performance of game solving algorithms that use BDDs as the symbolic data structure. 

Optimal variable orderings may be found using exact algorithms, but these are prohibitively expensive for BDDs with more than a few nodes. In practice heuristics are used which produce good, but not optimal orderings. One such heuristic is Ruddell's sifting algorithm \cite{sifting}.

The CUDD BDD package performs \emph{dynamic variable ordering}, which means that once the number of BDD nodes the package knows about grows past a certain threshold, the package automatically performs the requested reordering algorithm on all BDDs that exist in the manager. Dynamic variable ordering is critical to the performance of game solving algorithms that utilise BDDs and therefore we always enable it.

