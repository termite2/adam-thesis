\documentclass{book}

\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{amssymb}

\title{Device Driver Synthesis}
\author{Adam Walker}

\newcommand{\buchi}{Buchi }
\newcommand{\reach}[0]{\textsc{Reach}}
\newcommand{\safe}[0]{\textsc{Safe}}
\newcommand{\concrete}[1]{#1\mathord{\downarrow}}
\newcommand{\abstractm}[1]{#1\mathord{\uparrow^m}}
\newcommand{\abstractM}[1]{#1\mathord{\uparrow^M}}

\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}

\section{Termite}
\section{Contributions}
\section{Chapter outline}

\chapter{Background}

\section{Two player games}

Two-player games are a useful formalism for reactive synthesis. Many problems in electronic design automation, industrial automation and robotics can be formalised as a game. In particular, the driver synthesis problem can be formalised as a game, and, this is the formalism around which Termite is built. Here, we present the fundamentals of $\omega$ regular games. 

\subsection{Formalism}

A two player game is played by player~1 against its opponent, player~2. It consists of a possible infinite state space $S$ on which the game is played. The game is always in some state $s \in S$ called the current state. The game progresses from state to state according to a transition relation, $\delta \subseteq S \times L \times S$ where $S$ is the set of states and $L$ is a set of label variables. A transition $t \in S \times L \times S$ is allowed in the game iff $t \in delta$. 

The meaning of the label $l \in L$ depends on the type of game, but for now we will consider turn based games. In a turn based game, $S$ is partitioned into two sets: the player~1 set $\tau_1$ and the player~2 set $\tau_2$, where $\tau_1 \cap \tau_2 = \emptyset$ and $ \tau_1 \cup \tau_2 = S$. When $s \in \tau_1$ player 1 gets to pick $l$ and when $s \in \tau_2$ player 2 gets to pick $l$.

Lastly, each game has an associated set of initial states $I \in 2^S$ where execution of the game begins.

Putting this all together, we can identify a \emph{turn based game structure} $G = \langle S,L,I,\tau_1,\tau_2,\delta \rangle$ with a turn based game.

A game proceeds in an infinite sequence of rounds, starting from some initial state. The infinite sequence of states visited $(s_0, s_1,\ldots) \in S^\omega$ is called a path. 

A game is won by player 1 if, by picking suitable labels, they can force the path to be within a winning set of state objectives $\Phi \subseteq S^\omega$, where $S^\Omega$ is the set of all infinite sequences of states in $S$. An arbitrary set of infinite sequences is an extremely general, but not practically useful, way of defining an objective. In the following sections we will consider some more restricted objectives that have practical uses.

\subsection{Safety and reachability games}

The two simplest objectives are safety and reachability. A safety objective is defined by a set $SAFE \subseteq S$ that player 1 must force the game to stay within, regardless of the labels that player 2 picks. Formally, a run is safe if $\forall i. s_i \in SAFE$. 

The dual of a safety objective is a reachability objective. A reachability objective is defined by a set $REACH \subseteq S$ that player 1 must force the game to visit at least once, regardless of the labels that player 2 picks. Formally, a reachability run is winning if $\exists i. s_i \in REACH$

\subsection{Strategies, counterexamples}

Given an objective, a game is either winning for player~1 or it is losing. If it is winning, then there exists a strategy for player~1. Informally, a strategy tells player~1, in any state, which label it must play and ensures that if player~1 adheres to the strategy then the objective will be satisfied.

Formally, a strategy for player~1 is a function $\pi_1 : S^* \times \tau_1 \rightarrow L$ that, in any player~1 state, associates the history of the game with a label to play. It is known that for complete information games, as we are considering, if there exists a strategy, then there also exists one that only depends on the current state. Thus, we can simplify the definition of a strategy for player~1 to be a function $\pi_1 : \tau_1 \rightarrow L$ that only depends on the current state.

Conversely, if the game is losing for player~1, then there exists a spoiling strategy for player~2, that is, a strategy for player~2 that ensures that, no matter which labels player~1 picks, there is no way that player~1 can satisfy its objective.

Formally, a counterexample strategy for player~1 is a strategy for player~2 which is a function $\pi_2 : \tau_2 \rightarrow L$ that associates any player~2 state with a label for player~2 to play. Again, we have simplified the definition as there must exist a strategy that only depends on the current state. 

\subsection{Buchi, fairness and GR(1) games}
Here we consider objectives that are more complex and more useful in practice.

\paragraph{Buchi}
A Buchi objective is defined by a set $BUCHI \subseteq S$ that player~1 must always be able to force execution of the game into. This differs from a reachability game in that the region must always be reachable, not just once. When it has been reached once, it must be reachable again, and so on. So, it must be reachable infinitely many times. Formally, a run is buchi winning if $\forall i. \exists j>i. s_j \in BUCHI$

\paragraph{Generalized Buchi}
A Generalized Buchi objective is defined by a set of sets $BUCHIS \subseteq 2^S$. Player~1 must always be able to force execution into each set $BUCHI \in BUCHIS$. Formally, a run satisfies a generalized Buchi objective if $\forall i. \forall BUCHI \in BUCHIS. \exists j>i. s_j \in BUCHI$.

\paragraph{Fairness}
Sometimes it is necessary to rule out invalid plays that are not easily ruled out by changing the state machine. For this we use fairness conditions. A fairness condition is a set of states which we may assume that all valid runs of the game eventually leave. If a spoiling strategy exists that results in an unfair run, it does not count. Formally, a run satisfies a fair reachability objective if $\forall i. \exists j>i. s_j \notin UNFAIR \rightarrow \exists i. s_i \in REACH$. 

\paragraph{GR(1)}
A Generalized Reactive 1, or GR(1) objective, is a generalized Buchi objective with multiple fairness conditions. In practice, this turns out to be a very useful type of objective. Formally, a run satisfies a GR(1) objective if $\forall i. \forall UNFAIR \in UNFAIRS \exists j>i. s_j \notin UNFAIR \rightarrow \forall i. \forall BUCHI \in BUCHIS. \exists j>i. s_j \in BUCHI$.

\subsection{Solving games}
\subsubsection{Controllable predecessor}

All of the synthesis algorithms for games I will be describing use a function called the controllable predecessor (abbreviated CPre). CPre is a function from a set of game states ($2^S$) to another set of game states. Given a set $T$, $CPre(T)$ returns the set of states from which player 1 can force execution into $T$ in one step. 

The exact details of $CPre$ do not matter and it can be considered to be a parameter to the game solving algorithms. The function it computes depends on the application and I will be describing the driver synthesis $CPre$ later. For now, the only property of $CPre$ that we require is that it is monotonic, ie. if $X \subseteq Y$ then $CPre(X) \subseteq CPre(Y)$. Clearly, any reasonable $CPre$ function will have this property.

\subsubsection{Safety and reachability}

We will start with reachability as it is the easiest to understand. A state is winning in a reachability game if there is some finite $i$ such that we can guarantee that after $i$ rounds of the game, execution will have, at least once, entered a state in $REACH$.

We can define the winning region inductively. It is obviously possible to reach the set $REACH$ from $REACH$ in 0 steps as we are already there. This is the base case. It is also possible to reach $REACH$ from $x \in S$ in $N + 1$ steps or fewer iff it is possible to reach $REACH$ from all of $Y \subseteq 2^S$ in $N$ steps or fewer and $x \in CPRE(Y)$.

This suggests an algorithm. We start with $REACH$, apply $CPRE$ to get the states winning in 1 step, combine with $REACH$ again to get the states reachable in 1 step or less, and then repeat. On each iteration we find the states winning in $N$ or less steps. Two questions remain: 

\begin{itemize}
    \item An algorithm must terminate, so, when should we stop iterating?
    \item After termination, has the algorithm found all the winning states?
\end{itemize}

Observe that $W_{N+1} \supseteq W_N$. So, on each iteration, we either grow the winning set or it remains the same. Also observe that our set $S$ is finite, which means that $2^S$ is also finite. As set inclusion is a transitive relation, we cannot grow our winning set forever, so eventually we must reach a fixed point of the CPRE function. 

%TODO: $CPRE$ isnt the order preserving function
Or, if you prefer, the power set of $S$ can be ordered by set inclusion to obtain a complete lattice with supremum $S$ and infimum $\emptyset$. $CPRE$ is an order preserving function, so by the Knaster-Tarski theorem the set of fixed points of $CPRE$ is also a complete lattice. Thus there exists a greatest and least fixed point (as the lattice is complete). The least fixed point of $CPRE$ is clearly obtained by iteration starting from the least element of the lattice, ie. $\emptyset$.

Thus, we will eventually reach a fixed point, and, this is when we should stop iterating as further iterations will not change the winning set. Furthermore, we know that after any finite number $N$ of iterations where $N$ is greater than the number of iterations required to reach the fixed point, the winning region will remain $WIN$. Thus $WIN_N = WIN$ and we have found all winning states.

Safety games are the dual of reachability games and are also solved by iterating the controllable predecessor. They will not be described here.

\begin{equation}
REACH(T) = \mu X. CPre(X \vee T)
\label{equ:mu_reach}
\end{equation}

\begin{equation}
SAFE(T) = \nu X. CPre(X \wedge T)
\label{equ:mu_safe}
\end{equation}

\begin{algorithm}[t]
\begin{algorithmic}
\Function{Reach}{$REACH$}
\State $Y \gets \varnothing$
\Loop
\State $Y' \gets CPre(Y \cup REACH)$
\If {$Y' = Y$} 
\State\Return $Y$\EndIf
\State $Y \gets Y'$
\EndLoop
\EndFunction
\end{algorithmic}
\caption{Solving a reachability game}
\label{a:reach}
\end{algorithm}

\begin{algorithm}[t]
\begin{algorithmic}
\Function{Safe}{$SAFE$, $CPre$}
\State $Y \gets S$
\Loop
\State $Y' \gets CPre(Y \cap SAFE)$
\If {$Y' = Y$} 
\State\Return $Y$\EndIf
\State $Y \gets Y'$
\EndLoop
\EndFunction
\end{algorithmic}
\caption{Solving a safety game}
\label{a:safe}
\end{algorithm}

\subsubsection{GR(1)}

We will build on our algorithm for reachability games to solve Buchi and then GR(1) games. To solve a Buchi game, you first find the set from which you can reach the goal once as you do for a reachability game. Then, you use this set to find the set from which you can reach the goal twice, three times, and so on until you get to another fixed point. 

Imagine you have solved the reachability game $R = REACH(T)$ where $T$ is the Buchi target set and $R$ is the winning region. Then $V = R \wedge T$ is a subset of the goal from which you can reach the goal one more time. Computing $W = REACH(V)$ gets you the set of states from which you can reach the goal twice. Iterating this procedure will eventually lead to a fixed point as $REACH$ is a monotonic function.

This fixed point is the set of states from which you can reach the goal any number of times. Thus, it is the winning set of the Buchi game. The algorithm is given in algorithm \ref{a:buch}.

\begin{algorithm}[t]
\begin{algorithmic}
\Function{Buchi}{$T$, $CPre$}
\State $Y \gets S$
\Loop
\State $Y' \gets \Call{Reach}{Y \cap T}$
\If {$Y' = Y$} 
\State\Return $Y$\EndIf
\State $Y \gets Y'$
\EndLoop
\EndFunction
\end{algorithmic}
\caption{Solving a \buchi game}
\label{a:buchi}
\end{algorithm}

\begin{equation}
BUCHI(T) = \nu X. \mu Y. CPre(Y \vee (X \wedge T))
\label{eqn:mu_buchi}
\end{equation}

Next, we need to add fairness. Consider a fair reachability game. An unfair region is a region that we can assume execution will leave, regardless of the loops it contains. We modify the controllable predecessor to create a fair controllable predecessor that takes this into account. Intuitively, the algorithm considers a block of unfair states to be winning if the only way out leads to an already winning state. To achieve this, we play a variation of a safety game where we can win if we stay entirely within the unfair set or upon exiting the unfair set we are immediately in the target set $T$. The procedure for the fair controllable predecessor is given in algorithm \ref{a:fair_cpre}. Using $Fair\_CPre$ as the $CPre$ operator in the reachability algorithm (algorithm \ref{a:reach}) yields the fair reachability algorithm. The reader is invited to check that this algorithm correctly determines that all states in figure \ref{fig:fair} are winning.

\begin{algorithm}[t]
\begin{algorithmic}
\Function{Fair\_CPre}{$CPre$, $T$}
\State $Y \gets S$
\Loop
\State $Y' \gets \Call{CPre}{Y \cap T}$
\If {$Y' = Y$} 
\State\Return $Y$\EndIf
\State $Y \gets Y'$
\EndLoop
\EndFunction
\end{algorithmic}
\caption{The fair controllable predecessor}
\label{a:fair_cpre}
\end{algorithm}

\begin{equation}
FAIR\_CPRE(U, T) = \nu X. CPre((U \wedge X) \vee T)
\label{eqn:mu_fair}
\end{equation}

Finally, if we combine the Buchi game with the fair controllable predecessor we get a GR(1) game. The procedure is given in algorithm \ref{a:gr1}.

\begin{algorithm}[t]
\begin{algorithmic}
\Function{GR(1)}{$CPre$, $T$}
\State\Return \Call{Buchi}{$T$, Fair\_Cpre(CPre)}
\EndFunction
\end{algorithmic}
\caption{GR(1) game}
\label{a:gr1}
\end{algorithm}

\begin{equation}
GR1(G, U) = \nu X. \mu Y. \nu Z. CPre((U \wedge Z) \vee (G \wedge X) \vee Y)
\label{equ:mu_gr1}
\end{equation}

\subsection{Extracting strategies}

\subsection{Extracting counterexamples}

\section{Symbolic games}

\subsection{Symbolic algorithms}
\subsection{Strategy generation}
\subsection{Binary decision diagrams}

\chapter{Related work}
\section{Fault tolerance}
\section{Static analysis}
\section{Other device driver synthesis projects}
\section{Reactive synthesis}

\chapter{Games}


As a concrete example, we could create a crude formalism for driver synthesis using only a reachability game. Consider, for example, figure \ref{fig:reach}, which shows the state machine for a game to control a hypothetical network controller. Solid lines indicate controllable transitions and dashed lines indicate uncontrollable transitions. Execution begins in the leftmost state where the OS may initiate a network transfer by choosing the `send' label. The goal of the game is the rightmost state (labelled `G') as this is the point where player 1 has completed the request. So, to win, player 1 (who controls the transitions with solid lines) must ensure that execution of the state machine reaches the goal. 

\begin{figure}[t]
\centering
\includegraphics{diagrams/reachgame.pdf}
\caption{Reachability game for simple network device}
\label{fig:reach}
\end{figure}

The network device has two 8-bit registers, command (abbreviated cmd) and data. Writing 0x01 to the command register starts the transfer, and eventually whatever is in the data register gets written out to the network. Note that the actual sending of the data is an uncontrollable event. 

The correct sequence to win the game, therefore, is to write the data register and then the control register after the OS performs a send request. This takes us to state `S5' where the only move by player 2 is `evt\_send' taking us to the goal. 

If the command register is written first and then the data register there is potential for the environment to play the `evt\_send' label before the data is written, potentially resulting in the wrong data being sent. This is the transition that terminates in the `E' state (for error). The `E' state is a dead end, so it is not possible to reach the goal. 

So, if player 1 takes the top half of the diamond (ie. writes data before command) then it will be guaranteed to reach the goal and the reachability game is winning for player 1. The strategy to reach the goal tells us the sequence of labels the driver must play to get to the goal. In principle, this could be turned into a driver for our simple network device.

\section{\buchi, fairness and GR(1) games}

This simplistic formalism for driver synthesis has several shortcomings that we will deal with in the following sections.

\subsection{We must be able to repeatedly satisfy the OS requests}

Consider a simplified network controller that does not have a command register. Instead, writing to the data register triggers transmission of the byte. However, there are two ways of writing to the data register. One is a standard register write. The other also performs the register write and then schedules a self destruct sequence to happen immediately after the byte is transmitted. The state machine for this device is shown in figure \ref{fig:buchi}. The goal, in this case, is the set ${S3, S5}$ corresponding to the state after completion of the send request. The problem is that, unless you only ever want to send one byte, this goal does not capture the required behavior. One could easily work around this problem by specifying only ${S3}$ as the goal, 

The solution is to modify the objective of the game. Instead of being able to reach the goal once, we want to be able to reach the goal an infinite number of times. Or, equivalently, we want to always be able to reach the goal again. This kind of objective is called a Buchi objective and a game with a Buchi objective is called a Buchi game. 

\begin{figure}[t]
\centering
\includegraphics{diagrams/buchigame.pdf}
\caption{Buchi game for simple network device}
\label{fig:buchi}
\end{figure}

\subsection{We must be able to rule out invalid behaviors not easily expressed with state machines}

Consider a modification of our simplified network device without a self destruct sequence, but with the ability to check that noone is using the communication medium prior to transmitting. The state machine of this device is given in figure \ref{fig:fair}. After the user requests data transmission by writing to the data register, it executes a loop that checks if the medium is free, and if so, it performs the transmission. 

If we pose this as a reachability game with goal state $G$, then the game is not winnable. The device may stay in the loop forever as it is never guaranteed to exit. Such a behavior should not prevent a driver from being synthesized providing that we have good reason to believe that the loop will eventually exit. Looping forever can be seen as a invalid behavior and we want to synthesize a driver for this system providing the invalid behavior does not occur. 

In model checking these behaviors are eliminated with fairness conditions. Fairness conditions are sets of states which we guarantee will eventually be left, which we refer to as unfair states. In the example, the unfair states are the set ${S2, S3}$. The fairness condition says that we will eventually leave the unfair set, and the only way of doing this is through the $evt\_send$ transition, and the game becomes winning.

\begin{figure}[t]
\centering
\includegraphics{diagrams/fairreach.pdf}
\caption{Fair reachability game for simple network device}
\label{fig:fair}
\end{figure}

\section{Game based formalism for drivers}

The combination of fairness and buchi objectives is called a GR(1) objective. Intuitively a GR(1) objective says that we can always reach some goal state provided that we do not get stuck forever in some unfair set of states. We use GR(1) objectives in Termite as we have found that in practice it is sufficient to express our goals.

\section{Solving games}

We now have a formalism for the driver synthesis problem as a game. A practical driver synthesis tool using the game formalism must be able to solve and find strategies for these games for real device and operating system specifications in a reasonable amount of time. The principle challenge of this work is creating a synthesis algorithm that scales well enough to handle the large state machines of real device and operating system specifications. 

In this section I will introduce the basic synthesis algorithms for games of increasing complexity. Then, I will explain the techniques I use to handle large state spaces, namely untracked variable abstraction and predicate abstraction.


\section{State variable encoding}

\section{Binary decision diagrams}

The algorithm described so far appears very inefficient. Consider a reachability game. We are performing a backwards breadth-first search starting from $REACH$. If we were to implement it directly as described, we would need a set abstract datatype to represent the winning set. Some of the games we have solved with Termite have upwards of $2^{80}$ states, even after abstraction. Clearly, explicitly representing the winning set will never succeed. 

Identical problems are encountered in model checking. The breakthrough that revolutionised model checking was to represent state sets implicitly as an equation over state variables that is true iff the state is in the set. 

\section{Synthesis competition}

BDD based solvers as I have described so far are remarkably efficient. To illustrate this point, I entered a simple BDD based solver into the reactive synthesis competition in 2014. The solver won the sequential realizability category (the only one in which it was entered). 

The solver computes fixed points in a straightforward manner as already described, however, it makes use of several BDD based optimisations that are critical to its performance. These optimisations are also critical to the performance of the Termite synthesis algorithm, so they are outlined here:

\begin{itemize}
    \item Dynamic variable ordering using the sifting algorithm.
    \item No next state variables
    \item Partitioned transition relations
    \item Simultaneous conjunction and quantification
\end{itemize}

\section{Abstraction}
An abstraction is a simplification of the original transition system. An abstraction is used when the game is too large to be solved. Ideally, an abstraction is both small enough to be solved and detailed enough to gain some additional information about the properties of the system. 

One common use of an abstraction is in an abstraction-refinement loop. In an abstraction refinement loop, an initial simple abstraction is found and is solved. Then, the results of the abstraction are used to refine the abstraction, ie. to build another system model that contains slightly more detail than the original abstraction. This is repeated in a loop until the original game is solved. It is often possible to solve the original game with a far less detailed abstraction that the original system. 

Finding an abstraction that is simultaneously small and useful for making progress in solving the game is a difficult task and is what will be dealt with in the following sections. We start with earlier work on three valued abstraction refinement. 

\section{Three valued abstraction refinement}

The idea is that given an abstraction, we classify states into one of three categories: winning, losing, and unknown. If we discover that the entire initial set is winning, we know that the original game is winning and we can terminate. Dually, if we discover any initial state that is losing, we know that the entire initial set can never be winning, hence the game is losing and we can terminate. 

At termination, either 
\begin{itemize}
\item all of the initial states are classified as winning (but the other states need not be classified), or
\item one of the initial states is classified as losing (again, no other states need to be classified)
\end{itemize}

This additional imprecision often allows us to use a less precise abstraction compared to the original algorithm where all states are exactly classified. 

We need to describe what an abstraction actually is and how we use this abstraction to classify states. 

\subsection{Abstraction}

An abstraction of a game structure $G$ is a tuple $\langle V, 
\concrete{}\rangle$, where $V$ is a finite set of abstract states 
and $\concrete{} : V \rightarrow 2^S $ is the \emph{concretisation 
function}, which takes an abstract state and returns the possibly 
empty set of concrete states that the abstract state corresponds 
to.  We require that $\bigcup_{v\in V}\concrete{v} = S$ and         
$\concrete{v_1}\cap \concrete{v_2} = \emptyset$ for any $v_1$ and 
$v_2$, $v_1 \neq v_2$. In the case when $\concrete{v} = \emptyset$ 
the abstract state $v$ is said to be \emph{inconsistent}.  We 
extend the $\concrete{}$ operator to sets of abstract states.  For 
$U\subseteq V$: $\concrete{U} = \bigcup_{u\in U}\concrete{u}$.

\subsection{Algorithm}
In this section we present a modified version of the three-valued 
abstraction refinement technique of de~Alfaro and 
Roy~\cite{Alfaro_Roy_07}.  To simplify the presentation, we focus 
on solving reachability games.  De~Alfaro and Roy present an 
extension of their method to arbitrary $\omega$-regular games.  
This extension is directly applicable to the version of the 
algorithm presented here.

%Given an abstraction $\langle V, \concrete{}\rangle$ of a game 
%$G$, the three-valued abstraction refinement scheme computes 
%over- and under-approximations of the winning region of the game.  
%If necessary, the abstraction is refined in order to narrow down 
%the gap between the two.
We start with defining two versions of the abstraction operator: 
the \emph{may-abstraction} $\abstractm{}$ and the 
\emph{must-abstraction} $\abstractM{}$. For a set of concrete 
states $T \subseteq S$:
$\abstractm{T} = \{v\in V\mid \concrete{v} \cap T \neq 
\emptyset\}$, $\abstractM{T} = \{v\in V\mid \concrete{v} \subseteq 
T \}$.
We say that abstraction is \emph{precise} for a set $T\subseteq S$ 
if $\concrete{(\abstractm{T})} = \concrete{(\abstractM{T})}$.

Next, we define may and must versions of the abstract controllable 
predecessor operator:
\begin{equation}
    \small
    Cpre_i^m(U) = \abstractm{Cpre_i(\concrete{U})},~~
    Cpre_i^M(U) = \abstractM{Cpre_i(\concrete{U})}
\label{e:cpremM}
\end{equation}
These operators have the property:
$\concrete{Cpre_i^M(U)} \subseteq Cpre_i(\concrete{U}) \subseteq \concrete{Cpre_i^m(U)},$ and hence
$\concrete{\reach(\abstractM{T}, Cpre_i^M)} \subseteq \reach(T, Cpre_i) \subseteq \concrete{\reach(\abstractm{T}, Cpre_i^m)}.$

The abstract $Cpre_i^m$ and $Cpre_i^M$ operators are defined in 
terms of the concrete controllable predecessor $Cpre$. As these 
may not be possible to compute efficiently in practice, we 
introduce approximate versions, $Cpre_i^{m+}$ and $Cpre_i^{M-}$, 
such that for all $U\subseteq V$: $\concrete{Cpre_i^m(U)} 
\subseteq \concrete{Cpre_i^{m+}(U)}$ and 
$\concrete{Cpre_i^{M-}(U)}
\subseteq \concrete{Cpre_i^M(U)}.$  The definition of 
$Cpre_i^{m+}$ and $Cpre_i^{M-}$ is determined by each particular 
instantiation of the abstraction refinement scheme.  We present 
our version of these operators in Section~\ref{s:cpre}.  

Figure~\ref{f:reach} illustrates the main idea of our approach, 
which is presented in algorithm~\ref{alg:generic}.  At every 
iteration, the algorithm computes the must-winning set $W^M$ that 
underapproximates, and the may-winning set $W^m$ that 
overapproximates the true winning set (lines~2--3).  The algorithm 
terminates if the must-winning set contains the entire initial set 
or the may-winning set has shrunk beyond the initial set 
(lines~4--5).  Otherwise, the algorithm refines the abstraction in 
a way that expands the must-winning set.
%To this end we identify a set of states from 
%which player~1 can force the game to $W^M$ in one step.  Since all 
%states in $W^M$ are must-winning, the new state will be 
%must-winning as well.  Furthermore, due to the nature of the 
%reachability game, all winning states can eventually be discovered 
%in this way.  

\begin{algorithm}[t]
\caption{Three-valued abstraction refinement for games.}
\label{alg:generic}

\begin{algorithmic}[1]

% \Function{Solve}{$transitionRelation$, $goal$}
    \Statex {\bf Input:} A game structure $G = \langle S, L, I, \tau_1, \tau_2, \delta \rangle$, a set 
    of target states $T\subseteq S$, and an initial abstraction $\alpha=\langle V, \concrete{}, Cpre_1^{m+}, Cpre_1^{M-} \rangle$
    that is precise for $T$, $I$, and $\tau_i$.

    \Statex {\bf Output:} {\it Yes} if $I \subseteq \reach(T, Cpre_1)$, and {\it No} otherwise.

    \Loop
        \State $W^M \gets \reach(\abstractM{T}, Cpre_1^{M-})$
        \State $W^m \gets \reach(\abstractm{T}, Cpre_1^{m+})$
        \If{$\abstractM{I} \subseteq W^M$} 
            \State\Return Yes
        \ElsIf{$\abstractM{I} \nsubseteq W^m$} 
            \State\Return No
        \Else       
            \State $refined \gets \Call{refineCpre}{W^M}$
            \If {$(\neg refined)$}
                \State$\Call{refineAbstraction}{W^M}$
            \EndIf
        \EndIf
    \EndLoop
\end{algorithmic}
\end{algorithm}

The key observation behind the refinement procedure is that 
candidate winning states can be found at the \emph{may-must 
boundary} of the game, i.e., the set $Cpre_1^{m+}(W^M)\setminus 
W^M$, of all may-predecessors of the must-winning set.  The 
boundary consists of three regions shown in Figure~\ref{f:reach}: 
(1) $Cpre_1^{M}(W_M)\setminus W_M$, (2) $Cpre_1^m(W^M)\setminus 
Cpre_1^{M}(W^M)$, and (3) $Cpre_1^{m+}(W^M)\setminus 
Cpre_1^{m}(W^M)$.  The first and the third regions can be shrunk 
by increasing the precision of the $Cpre^{M-}$ and $Cpre^{m+}$ 
operators respectively.  The second region can only be shrunk by 
refining the abstraction itself, i.e., partitioning abstract 
states into smaller regions.

These two types of refinement are performed in lines~7 and~8 of 
the algorithm.  The \textsc{refineCpre} function computes a more 
precise version of the controllable predecessor operators.  It 
returns $false$ iff no such refinement is possible, i.e., 
$Cpre^{M}(W_M)=Cpre^{M-}(W_M)$ and $Cpre^{m+}(W^M)=Cpre^{m}(W^M)$.  
The \textsc{refineAbstraction} function refines the abstract state 
space in a way that expands the set $Cpre^M(W^M)$ with at least 
one new abstract state.  
%The repeated application of \textsc{refineCpre} 
%and \textsc{refineAbstraction} has the effect of eventually 
%extending the must-winning region $W^M$ with new states, which 
%guarantees termination of the algorithm.

Algorithm~\ref{alg:generic} differs from \cite{Alfaro_Roy_07}
in that it uses an additional type of refinement which refines the 
controllable predecessor operators without changing the abstract
state space.

%Use reachability
%The 3 valued abstraction refinement paper uses an abstraction that is precise wrt the initial state. We dont need this. What is precise?
%Iteratively classify states in may but not must as winning or not.
%Describe it

\subsection{An improved symbolic implementation}

From an email to the german guys:

It's a simple abstraction-refinement loop. At the start, the safety condition is compiled to a BDD. In the master branch of my solver, I then compile update functions for each state variable. In the "untracked" branch, I only compile update functions for each state variable that occurs in the safety condition. However, these update functions may depend on additional state variables. Instead of compiling update functions for these additional state variables, I make them input variables. I refer to these as untracked variables. The state space of the game is now much smaller. The player (ie. not the environement) is free to choose the values of these inputs. This makes the game easier to win for the player.

The simplified game is then solved. If the game is lost, then the player certainly loses. If the player wins, it may because the abstraction made it easer to win the game. To detect this, we do the controllable predecessor one more time, but dont quantify out the untracked variables. We pick a small losing <state, untracked> cube and change the untracked variables to state variables by computing their update functions, and solve the game again. If no such cube exists, the game is winning.

On each refinement iteration, the winning region can only shrink, so we reuse the winning region from solving the last game on the next iteration. Also, if we were solving a reachability game, we would let the environment pick the valuations of the untracked variables and the winning sets would only grow.

In practice, this works very well for the games I am solving to synthesize controllers for computer hardware. It does not work so well on the synthesis competition benchmarks because most of the state is needed to determine whether the game is winning, though, for some reason, it can create simpler abstractions for a lot of the unrealizable benchmarks. 

\subsection{Extension to GR(1) games and correctness proof}

\section{Predicate abstraction}
\subsection{Motivation}
\subsection{Algorithm}

\chapter{Termite}

\section{Specifications}
\section{Heuristic code generation}
\section{User guided code generation}

\section{Counterexample guided debugging}
\section{Limitations}

\section{A realistic example}

\section{Case studies}

\chapter{Conclusions}

\chapter{Appendix}

\end{document}
